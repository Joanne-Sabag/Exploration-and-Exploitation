import numpy as np
import matplotlib.pyplot as plt
from combination_lock_simulator import make_combination_lock
from UCBVI import UCBVI
from epsilon_greedy import EPS_GREEDY
from UCBH import UCBH

S = 5
A = 2
H = 10

env_opt = make_combination_lock(epLen=H, nState=S, nAction=A)  # Simulation of optimal policy - always go right
env_ucbh = make_combination_lock(epLen=H, nState=S, nAction=A)
env_eg = make_combination_lock(epLen=H, nState=S, nAction=A)
env_ucbvi = make_combination_lock(epLen=H, nState=S, nAction=A)

K = 1000  # number of episodes

agent_ucbh = UCBH(env_ucbh, K)
agent_eg = EPS_GREEDY(env_eg, K)
agent_ucbvi = UCBVI(env_ucbvi, K)

reward_ucbh = 0
reward_eg = 0
reward_ucbvi = 0
reward_opt = 0
rewards_ucbh = []
rewards_eg = []
rewards_ucbvi = []
rewards_opt = []
V_pi_ucbh = []
V_pi_eg = []
V_pi_ucbvi = []
V_pi_opt = []

for k in range(K):
    print(k, '/', K)
    env_ucbh.reset()
    env_eg.reset()
    env_ucbvi.reset()
    env_opt.reset()
    done_ucbh = 0
    done_eg = 0
    done_ucbvi = 0

    while done_ucbh != 1:
        s_ucbh = env_ucbh.state
        h_ucbh = env_ucbh.timestep
        a_ucbh = agent_ucbh.act()

        s_eg = env_eg.state
        h_eg = env_eg.timestep
        a_eg = agent_eg.act()

        s_ucbvi = env_ucbvi.state
        h_ucbvi = env_ucbvi.timestep
        a_ucbvi = agent_ucbvi.act()

        s_opt = env_opt.state
        h_opt = env_opt.timestep
        a_opt = 0

        r_ucbh, s_ucbh_, done_ucbh = env_ucbh.advance(a_ucbh)
        agent_ucbh.learn(a_ucbh, r_ucbh, s_ucbh_)

        r_eg, s_eg_, done_eg = env_eg.advance(a_eg)
        agent_eg.learn(a_eg, r_eg, s_eg_)

        r_ucbvi, s_ucbvi_, done_ucbvi = env_ucbvi.advance(a_ucbvi)
        agent_ucbvi.update_buffer(s_ucbvi, a_ucbvi, r_ucbvi, s_ucbvi_, h_ucbvi)

        r_opt, s_opt_, done_opt = env_opt.advance(a_opt)

        rewards_ucbh.append(r_ucbh)
        rewards_eg.append(r_eg)
        rewards_ucbvi.append(r_ucbvi)
        rewards_opt.append(r_opt)

    agent_ucbvi.learn(k)

    # Evaluate performance at the end of each episode
    env_ucbh.reset()
    env_eg.reset()
    env_ucbvi.reset()
    env_opt.reset()
    done_ucbh = 0
    V_pi_k_ucbh = 0
    V_pi_k_eg = 0
    V_pi_k_ucbvi = 0
    V_pi_k_opt = 0
    while done_ucbh != 1:
        s_ucbh = env_ucbh.state
        h_ucbh = env_ucbh.timestep
        a_ucbh = agent_ucbh.greedy_action(s_ucbh, h_ucbh)

        s_eg = env_eg.state
        h_eg = env_eg.timestep
        a_eg = agent_eg.greedy_action(s_eg, h_eg)

        s_ucbvi = env_ucbvi.state
        h_ucbvi = env_ucbvi.timestep
        a_ucbvi = agent_ucbvi.greedy_action(s_ucbvi, h_ucbvi)

        s_opt = env_opt.state
        h_opt = env_opt.timestep
        a_opt = 0

        r_ucbh, s_ucbh_, done_ucbh = env_ucbh.advance(a_ucbh)
        r_eg, s_eg_, done_eg = env_eg.advance(a_eg)
        r_ucbvi, s_ucbvi_, done_ucbvi = env_ucbvi.advance(a_ucbvi)
        r_opt, s_opt_, done_opt = env_opt.advance(a_opt)

        V_pi_k_ucbh += r_ucbh
        V_pi_k_eg += r_eg
        V_pi_k_ucbvi += r_ucbvi
        V_pi_k_opt += r_opt

    V_pi_ucbh.append(V_pi_k_ucbh)
    V_pi_eg.append(V_pi_k_eg)
    V_pi_ucbvi.append(V_pi_k_ucbvi)
    V_pi_opt.append(V_pi_k_opt)

np.save("outputs\\rewards_ucbh" + "_H_" + str(H) + "_K_" + str(K) + "_S_" + str(S) + "_rewards.npy", np.array(rewards_ucbh))
np.save("outputs\\rewards_eg" + "_H_" + str(H) + "_K_" + str(K) + "_S_" + str(S) + "_rewards.npy", np.array(rewards_eg))
np.save("outputs\\rewards_ucbvi" + "_H_" + str(H) + "_K_" + str(K) + "_S_" + str(S) + "_rewards.npy", np.array(rewards_ucbvi))
np.save("outputs\\rewards_opt" + "_H_" + str(H) + "_K_" + str(K) + "_S_" + str(S) + "_rewards.npy", np.array(rewards_opt))

np.save("outputs\\V_pi_ucbh" + "_H_" + str(H) + "_K_" + str(K) + "_S_" + str(S) + "_rewards.npy", np.array(V_pi_ucbh))
np.save("outputs\\V_pi_eg" + "_H_" + str(H) + "_K_" + str(K) + "_S_" + str(S) + "_rewards.npy", np.array(V_pi_eg))
np.save("outputs\\V_pi_ucbvi" + "_H_" + str(H) + "_K_" + str(K) + "_S_" + str(S) + "_rewards.npy", np.array(V_pi_ucbvi))
np.save("outputs\\V_pi_opt" + "_H_" + str(H) + "_K_" + str(K) + "_S_" + str(S) + "_rewards.npy", np.array(V_pi_opt))

fig = plt.figure()
T = np.array(range(K * H))
# regret_bound = np.sqrt(H * H * S * A * T)
eps_regret_bound = np.minimum(T, np.ones(T.shape) * A**(H/2))
regret_ucbh = np.cumsum(rewards_opt) - np.cumsum(rewards_ucbh)
regret_eg = np.cumsum(rewards_opt) - np.cumsum(rewards_eg)
regret_ucbvi = np.cumsum(rewards_opt) - np.cumsum(rewards_ucbvi)

plt.plot(T, regret_ucbh, label='UCB-H')
plt.plot(T, regret_eg, label='$\epsilon$-Greedy')
plt.plot(T, regret_ucbvi, label='UCB-VI H')
# plt.plot(T, regret_bound, label='$\sqrt{H^2SAT}$')

plt.xlabel('Timestep')
plt.ylabel('Regret')
plt.title('A = ' + str(A) + ' | S = ' + str(S) + "| H = " + str(H) + "| K = " + str(K))

plt.legend()

plt.savefig(r"outputs\\" + "H_" + str(H) + "_K_" + str(K) + "_S_" + str(S) + "_A_" + str(A) + ".png")
###########################################
fig = plt.figure()
K_vec = np.array(range(K))
# regret_bound = np.sqrt(H * H * S * A * T)
# eps_regret_bound = np.minimum(T, np.ones(T.shape) * A**(H/2))
regret_ucbh = np.cumsum(V_pi_opt) - np.cumsum(V_pi_ucbh)
regret_eg = np.cumsum(V_pi_opt) - np.cumsum(V_pi_eg)
regret_ucbvi = np.cumsum(V_pi_opt) - np.cumsum(V_pi_ucbvi)

plt.plot(K_vec, regret_ucbh, label='UCB-H')
plt.plot(K_vec, regret_eg, label='$\epsilon$-Greedy')
plt.plot(K_vec, regret_ucbvi, label='UCB-VI H')
# plt.plot(T, regret_bound, label='$\sqrt{H^2SAT}$')

plt.xlabel('Timestep')
plt.ylabel('my Regret')
plt.title('A = ' + str(A) + ' | S = ' + str(S) + "| H = " + str(H) + "| K = " + str(K))

plt.legend()

plt.savefig(r"outputs\\regret_" + "H_" + str(H) + "_K_" + str(K) + "_S_" + str(S) + "_A_" + str(A) + ".png")

plt.show()
