import numpy as np
import matplotlib.pyplot as plt
from combination_lock_simulator import make_combination_lock
from UCBH import UCBH

# Ks = [100, 500, 1000, 5000, 10000, 50000, 100000, 500000, 1000000]
# for A in range(3, 20):
#     print('A = ', A)
#     for S in range(5, 1000, 10):
#         for H in range(10, 1000, 10):
#             for K in Ks:
#                 T = K * H
#                 if T >= (H * S ** 3 * A) and (S * A) >= H:
#                     eps_regret = np.min([T, A ** (H / 2)])
#                     ucb_regret = H ** 2 * np.sqrt(float(S * A * T))
#                     if eps_regret > ucb_regret:
#                         print('A = ', A, ' S = ', S, ' H = ', H, ' K = ', K)



S = 4
A = 5
H = 10

env_ucbh = make_combination_lock(epLen=H, nState=S, nAction=A)
env_opt = make_combination_lock(epLen=H, nState=S, nAction=A)

K = 100000  # number of episodes

results_list_ucbh =[]

p_values = [0.1, 0.01, 0.001]
c_values = [0.1, 0.01, 0.001]
# p_values = [0.1, 0.01]
# c_values = [10]

fig = plt.figure()
K_vec = np.array(range(K))
T = K_vec * H

for p in p_values:
    for c in c_values:
        agent_ucbh = UCBH(env_ucbh, K, c, p)

        V_pi_ucbh = []
        V_pi_opt = []

        for k in range(K):
            if k % 1000 == 0: print(k, '/', K)
            env_ucbh.reset()
            env_opt.reset()
            done_ucbh = 0

            while done_ucbh != 1:
                s_ucbh = env_ucbh.state
                h_ucbh = env_ucbh.timestep
                a_ucbh = agent_ucbh.act()

                s_opt = env_opt.state
                h_opt = env_opt.timestep
                a_opt = 0

                r_ucbh, s_ucbh_, done_ucbh = env_ucbh.advance(a_ucbh)
                agent_ucbh.learn(a_ucbh, r_ucbh, s_ucbh_)

                r_opt, s_opt_, done_opt = env_opt.advance(a_opt)

            # Evaluate performance at the end of each episode
            env_ucbh.reset()
            env_opt.reset()
            done_ucbh = 0
            V_pi_k_ucbh = 0
            V_pi_k_opt = 0
            while done_ucbh != 1:
                s_ucbh = env_ucbh.state
                h_ucbh = env_ucbh.timestep
                a_ucbh = agent_ucbh.greedy_action(s_ucbh, h_ucbh)

                s_opt = env_opt.state
                h_opt = env_opt.timestep
                a_opt = 0

                r_ucbh, s_ucbh_, done_ucbh = env_ucbh.advance(a_ucbh)
                r_opt, s_opt_, done_opt = env_opt.advance(a_opt)

                V_pi_k_ucbh += r_ucbh
                V_pi_k_opt += r_opt

            V_pi_ucbh.append(V_pi_k_ucbh)
            V_pi_opt.append(V_pi_k_opt)

        np.save("outputs_combLock\\V_pi_ucbh" + "_H_" + str(H) + "_K_" + str(K) + "_S_" + str(S) + "_c_" + str(
            c) + "_p_" + str(p) + "_rewards.npy", np.array(V_pi_ucbh))
        np.save("outputs_combLock\\V_pi_opt" + "_H_" + str(H) + "_K_" + str(K) + "_S_" + str(S) + "_c_" + str(
            c) + "_p_" + str(p) + "_rewards.npy", np.array(V_pi_opt))

        results_list_ucbh.append(np.cumsum(V_pi_opt) - np.cumsum(V_pi_ucbh))
        regret_ucbh = (np.cumsum(V_pi_opt) - np.cumsum(V_pi_ucbh))
        plt.plot(K_vec, regret_ucbh, label="c="+str(c)+" p="+str(p))

# ucbh_regret_bound = H ** 2 * np.sqrt(S * A * T)

# plt.plot(K_vec, ucbh_regret_bound, label='$\sqrt{H^4SAT}$', linestyle='dashed')

plt.xlabel('Episode')
plt.ylabel('Total Regret')
plt.title('A = ' + str(A) + ' | S = ' + str(S) + "| H = " + str(H) + "| K = " + str(K))

plt.legend()

plt.savefig(r"outputs_combLock\\parameters_" + "H_" + str(H) + "_K_" + str(K) + "_S_" + str(S) + "_A_" + str(A) + ".png")

plt.show()

