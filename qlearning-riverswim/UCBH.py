import numpy as np
import random
import copy


class UCBH(object):
    def __init__(self, env, K, c=0.01, p=0.01):
        self.env = env
        self.K = K
        self.buffer = {h: [] for h in range(self.env.epLen)}
        self.N = {(h, s, a): 0.0 for h in range(self.env.epLen + 1) for s in self.env.states.keys() \
                  for a in range(self.env.nAction)}
        # self.Q = {(h, s, a): self.env.epLen + 1 for h in range(self.env.epLen) for s in self.env.states.keys() \
        #           for a in range(self.env.nAction)}
        self.Q = {(h, s, a): self.env.epLen for h in range(self.env.epLen) for s in self.env.states.keys() \
                  for a in range(self.env.nAction)}
        self.c = c
        self.p = p
        self.iota = np.log(self.env.nState * self.env.nAction * self.env.epLen * self.K / self.p)
        self.learning_rate = 1

    # def update_buffer(self, s, a, r, s_, h):
    #     self.buffer[h].append((s, a, r, s_, h))

    def act(self):
        s = self.env.state
        h = self.env.timestep
        return self.greedy_action(s, h)

    def greedy_action(self, s, h):
        x = np.array([self.Q[(h, s, a)] for a in range(self.env.nAction)])
        return self.env.argmax(x)

    def learn(self, action, reward, s_):
        self.update_value_functions(action, reward, s_)

    def update_value_functions(self, action, reward, s_):
        self.N[(self.env.timestep, self.env.state, action)] += 1
        t = self.N[(self.env.timestep, self.env.state, action)]
        bonus = self.c * np.sqrt((self.env.epLen ** 3) * self.iota / t)
        self.learning_rate = (self.env.epLen + 1) / (self.env.epLen + t)

        old_value = self.Q[(self.env.timestep, self.env.state, action)]
        if self.env.timestep < self.env.epLen - 1:
            future_action = self.greedy_action(s_, self.env.timestep + 1)
            future_reward = min(self.env.epLen, self.Q[(self.env.timestep + 1, s_, future_action)])
        else:
            future_reward = 0

        new_value = (1 - self.learning_rate) * old_value + self.learning_rate * (reward + future_reward + bonus)
        self.Q[(self.env.timestep, self.env.state, action)] = min(new_value, self.env.epLen)
