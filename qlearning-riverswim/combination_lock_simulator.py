import numpy as np
import copy

class Environment(object):
    """General RL environment"""

    def __init__(self):
        pass

    def reset(self):
        pass

    def advance(self, action):
        """
        Moves one step in the environment.
        Args:
            action
        Returns:
            reward - double - reward
            newState - int - new state
            pContinue - 0/1 - flag for end of the episode
        """
        return 0, 0, 0


def make_combination_lock(epLen=20, nState=5, nAction=2):
    # nAction = 2
    R1_true = {}
    P1_true = {}
    R2_true = {}
    P2_true = {}
    states = {}
    for s in range(nState):
        states[(s)] = 0.0
        for a in range(nAction):
            R1_true[s, a] = (0, 0)
            P1_true[s, a] = np.zeros(nState)
            R2_true[s, a] = (0, 0)
            P2_true[s, a] = np.zeros(nState)

    # Rewards
    R2_true[0, 0] = (1, 0)

    # Transitions
    P1_true[0, 0][0] = 1
    for s in range(1, nState):
        for s_ in range(1, nState):
            P1_true[s, 0][s_] = 1 / (nState - 1)

    for s in range(nState):
        for a in range(1, nAction):
            for s_ in range(1, nState):
                P1_true[s, a][s_] = 1 / (nState - 1)

    for s in range(nState):
        for a in range(nAction):
            P2_true[s, a][s] = 1

    combinationLock = TabularMDP(nState, nAction, epLen)
    combinationLock.R1 = R1_true
    combinationLock.P1 = P1_true
    combinationLock.R2 = R2_true
    combinationLock.P2 = P2_true
    combinationLock.states = states
    combinationLock.reset()

    return combinationLock


class TabularMDP(Environment):
    """
    Tabular MDP
    R - dict by (s,a) - each R[s,a] = (meanReward, sdReward)
    P - dict by (s,a) - each P[s,a] = transition vector size S
    """

    def __init__(self, nState, nAction, epLen):
        """
        Initialize a tabular episodic MDP
        Args:
            nState  - int - number of states
            nAction - int - number of actions
            epLen   - int - episode length
        Returns:
            Environment object
        """

        self.nState = nState
        self.nAction = nAction
        self.epLen = epLen

        self.timestep = 0
        self.state = 0
        self.prev_state = 0

        # Now initialize R and P
        self.R1 = {}
        self.P1 = {}
        self.R2 = {}
        self.P2 = {}
        self.R = {}
        self.P = {}
        self.states = {}
        for state in range(nState):
            for action in range(nAction):
                self.R[state, action] = (1, 1)
                self.P[state, action] = np.ones(nState) / nState

    def reset(self):
        """Resets the Environment"""
        self.timestep = 0
        self.state = 0
        self.R = self.R1
        self.P = self.P1

    def advance(self, action):
        """
        Move one step in the environment
        Args:
        action - int - chosen action
        Returns:
        reward - double - reward
        newState - int - new state
        episodeEnd - 0/1 - flag for end of the episode
        """
        if self.R[self.state, action][1] < 1e-9:
            # Hack for no noise
            reward = self.R[self.state, action][0]
        else:
            reward = np.random.normal(loc=self.R[self.state, action][0],
                                      scale=self.R[self.state, action][1])
        # print(self.state, action, self.P[self.state, action])
        newState = np.random.choice(self.nState, p=self.P[self.state, action])

        # Update the environment
        self.prev_state = self.state
        self.state = newState
        self.timestep += 1

        if self.timestep >= (self.epLen / 2):
            self.R = self.R2
            self.P = self.P2

        episodeEnd = 0
        if self.timestep == self.epLen:
            episodeEnd = 1

        return reward, newState, episodeEnd

    def argmax(self, b):
        # print(b)
        return np.random.choice(np.where(b == b.max())[0])