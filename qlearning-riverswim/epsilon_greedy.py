import numpy as np


class EPS_GREEDY(object):
    def __init__(self, env, K):
        self.env = env
        self.K = K
        # self.Q = {(h, s, a): self.env.epLen + 1 for h in range(self.env.epLen) for s in self.env.states.keys() \
        #           for a in range(self.env.nAction)}
        self.Q = {(h, s, a): 0 for h in range(self.env.epLen) for s in self.env.states.keys() \
                  for a in range(self.env.nAction)}
        self.learning_rate = 0.05

    def act(self):
        s = self.env.state
        h = self.env.timestep

        x = np.array([self.Q[(h, s, a)] for a in range(self.env.nAction)])
        prob = 0.1
        choice = np.random.binomial(n=1, p=prob)
        rand_action = np.random.binomial(n=1, p=0.5)
        if choice:
            return rand_action
        else:
            return self.env.argmax(x)

    def greedy_action(self, s, h):
        x = np.array([self.Q[(h, s, a)] for a in range(self.env.nAction)])
        return self.env.argmax(x)

    def learn(self, action, reward, s_):
        self.update_value_functions(action, reward, s_)

    def update_value_functions(self, action, reward, s_):
        old_value = self.Q[(self.env.timestep, self.env.state, action)]
        if self.env.timestep < self.env.epLen - 1:
            future_action = self.greedy_action(s_, self.env.timestep + 1)
            future_reward = min(self.env.epLen, self.Q[(self.env.timestep + 1, s_, future_action)])
        else:
            future_reward = 0

        new_value = old_value + self.learning_rate * (reward + future_reward - old_value)
        self.Q[(self.env.timestep, self.env.state, action)] = min(new_value, self.env.epLen)
