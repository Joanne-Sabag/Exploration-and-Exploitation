import numpy as np
import matplotlib.pyplot as plt
from riverswim_simulator import make_riverSwim
from UCBVI import UCBVI
from epsilon_greedy import EPS_GREEDY
from UCBH import UCBH

S = 6
A = 2
H = 10

env_opt = make_riverSwim(epLen=H, nState=S)  # Simulation of optimal policy - always go right
env_ucbh = make_riverSwim(epLen=H, nState=S)
env_eg = make_riverSwim(epLen=H, nState=S)
env_ucbvi = make_riverSwim(epLen=H, nState=S)

K = 10000  # number of episodes

agent_ucbh = UCBH(env_ucbh, K)
agent_eg = EPS_GREEDY(env_eg, K)
agent_ucbvi = UCBVI(env_ucbvi, K)

reward_ucbh = 0
reward_eg = 0
reward_ucbvi = 0
reward_opt = 0
rewards_ucbh = []
rewards_eg = []
rewards_ucbvi = []
rewards_opt = []

for k in range(K):
    print(k, '/', K)
    env_ucbh.reset()
    env_eg.reset()
    env_ucbvi.reset()
    env_opt.reset()
    done_ucbh = 0
    done_eg = 0
    done_ucbvi = 0

    while done_ucbh != 1:
        s_ucbh = env_ucbh.state
        h_ucbh = env_ucbh.timestep
        a_ucbh = agent_ucbh.act()

        s_eg = env_eg.state
        h_eg = env_eg.timestep
        a_eg = agent_eg.act()

        s_ucbvi = env_ucbvi.state
        h_ucbvi = env_ucbvi.timestep
        a_ucbvi = agent_ucbvi.act()

        s_opt = env_opt.state
        h_opt = env_opt.timestep
        a_opt = 1

        r_ucbh, s_ucbh_, done_ucbh = env_ucbh.advance(a_ucbh)
        agent_ucbh.learn(a_ucbh, r_ucbh, s_ucbh_)

        r_eg, s_eg_, done_eg = env_eg.advance(a_eg)
        agent_eg.learn(a_eg, r_eg, s_eg_)

        r_ucbvi, s_ucbvi_, done_ucbvi = env_ucbvi.advance(a_ucbvi)
        agent_ucbvi.update_buffer(s_ucbvi, a_ucbvi, r_ucbvi, s_ucbvi_, h_ucbvi)

        r_opt, s_opt_, done_opt = env_opt.advance(a_opt)
        # print('h = ', env_opt.timestep, ' r = ', r_opt, ' state = ', env_opt.state)

        # reward_ucbh += r_ucbh
        rewards_ucbh.append(r_ucbh)
        # reward_eg += r_eg
        rewards_eg.append(r_eg)
        # reward_ucbvi += r_ucbvi
        rewards_ucbvi.append(r_ucbvi)
        # reward_opt += r_opt
        rewards_opt.append(r_opt)

    agent_ucbvi.learn(k)

np.save("outputs\\rewards_ucbh" + "_H_" + str(H) + "_K_" + str(K) + "_S_" + str(S) + "_rewards.npy", np.array(rewards_ucbh))
np.save("outputs\\rewards_eg" + "_H_" + str(H) + "_K_" + str(K) + "_S_" + str(S) + "_rewards.npy", np.array(rewards_eg))
np.save("outputs\\rewards_ucbvi" + "_H_" + str(H) + "_K_" + str(K) + "_S_" + str(S) + "_rewards.npy", np.array(rewards_ucbvi))
np.save("outputs\\rewards_opt" + "_H_" + str(H) + "_K_" + str(K) + "_S_" + str(S) + "_rewards.npy", np.array(rewards_opt))

fig = plt.figure()
T = np.array(range(K * H))
# regret_bound = np.sqrt(H * H * S * A * T)
eps_regret_bound = np.minimum(T, np.ones(T.shape) * A**(H/2))
regret_ucbh = np.cumsum(rewards_opt) - np.cumsum(rewards_ucbh)
regret_eg = np.cumsum(rewards_opt) - np.cumsum(rewards_eg)
regret_ucbvi = np.cumsum(rewards_opt) - np.cumsum(rewards_ucbvi)

plt.plot(T, regret_ucbh, label='UCB-H')
plt.plot(T, regret_eg, label='$\epsilon$-Greedy')
plt.plot(T, regret_ucbvi, label='UCB-VI H')
# plt.plot(T, regret_bound, label='$\sqrt{H^2SAT}$')

plt.xlabel('Timestep')
plt.ylabel('Regret')
plt.title('S = ' + str(S) + "| H = " + str(H) + "| K = " + str(K))

plt.legend()

plt.savefig(r"outputs\\" + "H_" + str(H) + "_K_" + str(K) + "_S_" + str(S) + ".png")

plt.show()
