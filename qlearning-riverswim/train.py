import numpy as np
import matplotlib.pyplot as plt
from tqdm.notebook import tqdm
from riverswim_simulator import make_riverSwim
from UCBVI import UCBVI
from epsilon_greedy import EPS_GREEDY
from UCBH import GREEDY

S = 6
A = 2
H = 15
env = make_riverSwim(epLen = H, nState = S)
env_opt = make_riverSwim(epLen = H, nState = S) # Simulation of optimal policy - always go right
env_g = make_riverSwim(epLen = H, nState = S)
env_eg = make_riverSwim(epLen = H, nState = S)
K = 10000
agent = UCBVI(env,K)
agent_g = GREEDY(env,K)
agent_eg = EPS_GREEDY(env,K)
reward = 0
reward_g = 0
reward_eg = 0
reward_opt = 0
rewards = []
rewards_g = []
rewards_eg = []
rewards_opt = []

for k in range(K):
    print(k, '/', K)
    env.reset()
    env_opt.reset()
    env_g.reset()
    env_eg.reset()
    done = 0
    done_g = 0
    done_eg = 0
    while done != 1:
        s = env.state
        h = env.timestep
        a = agent.act(s,h)
        s_g = env_g.state
        h_g = env_g.timestep
        a_g = agent_g.act(s_g,h_g,k,H)
        s_eg = env_eg.state
        h_eg = env_eg.timestep
        a_eg = agent_eg.act(s_eg,h_eg,k,H)
        s_opt = env_opt.state
        h_opt = env_opt.timestep
        a_opt = 1
        if k == K-1:
            print(a)
            #print(a_g)
        r,s_,done = env.advance(a)
        r_g,s_g_,done_g = env_g.advance(a_g)
        r_eg,s_eg_,done_eg = env_eg.advance(a_eg)
        r_opt,s_opt_,done_opt = env_opt.advance(a_opt)
        reward += r
        rewards.append(r)
        reward_g += r_g
        rewards_g.append(r_g)
        reward_eg += r_eg
        rewards_eg.append(r_eg)
        reward_opt += r_opt
        rewards_opt.append(r_opt)

        agent.update_buffer(s, a, r, s_, h)
        agent_g.update_buffer(s_g, a_g, r_g, s_g_, h_g)

    agent.learn(k)
    agent_g.learn(k)
    agent_eg.learn(k, a_eg, r_eg, s_)

font = {'fontname':'Times New Roman'}
fig = plt.figure()
T = np.array(range(K*H))
regret_bound = np.sqrt(H*S*A*T)
regret_ucbvi = np.cumsum(rewards_opt)-np.cumsum(rewards)
regret_greedy = np.cumsum(rewards_opt)-np.cumsum(rewards_g)
regret_egreedy = np.cumsum(rewards_opt)-np.cumsum(rewards_eg)
plt.plot(T,regret_ucbvi,label='UCB-VI BF')
plt.plot(T,regret_egreedy,label='$\epsilon$-Greedy')
plt.plot(T,regret_greedy,label='Explore First')
plt.plot(T,regret_bound,label='$\sqrt{HSAT}$')
plt.xlabel('Timestep',**font)
plt.ylabel('Regret',**font)
#plt.title('Linear SVM',**font)
plt.legend()
plt.show()