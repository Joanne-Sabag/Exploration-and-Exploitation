import random
import json
import argparse
import time
from epsilon_greedy import Eps_greedy
from UCBH import UCBH
from UCBVI import UCBVI
from dungeon_simulator import DungeonSimulator
import matplotlib.pyplot as plt
import numpy as np
import datetime


def main():
    # possible_H = [10, 50, 100]
    # possible_K = [100, 1000, 5000, 10000]
    possible_H = [10]
    possible_K = [10000]
    # possible_c = [0.01, 0.1]
    # possible_p = [0.01, 0.1]
    possible_c = [0.01]
    possible_p = [0.01]
    possible_delta = [0.3]
    # possible_delta = [1 / 3, 0.1, 0.5]

    for p in possible_p:
        for c in possible_c:
            for delta in possible_delta:
                for H in possible_H:
                    for K in possible_K:
                        print('p = ', p, ' | c = ', c, ' | delta = ', delta, ' | H = ', H, ' | K = ', K)
                        parser = argparse.ArgumentParser()
                        parser.add_argument('--agent', type=str, default='UCBVI', help='Which agent to use')
                        parser.add_argument('--learning_rate', type=float, default=0.1, help='How quickly the algorithm tries to learn')
                        parser.add_argument('--discount', type=float, default=1, help='Discount for estimated future action')
                        parser.add_argument('--episodes', type=int, default=K, help='Episodes count')  # K
                        parser.add_argument('--iterations', type=int, default=H, help='Iteration count')  # H
                        parser.add_argument('--const', type=int, default=c, help='const for UCB-H')
                        parser.add_argument('--p', type=float, default=p, help='p for UCB-H')
                        parser.add_argument('--delta', type=float, default=delta, help='delta for UCB-VI')
                        ARGS, unparsed = parser.parse_known_args()

                        # setup simulation
                        dungeon = DungeonSimulator()

                        # select agent
                        if ARGS.agent == 'EPS_GREEDY':  # model-free
                            agent = Eps_greedy(dungeon, learning_rate=ARGS.learning_rate, discount=ARGS.discount, iterations=ARGS.iterations)
                        elif ARGS.agent == 'UCBH':  # model-free
                            agent = UCBH(dungeon, const=ARGS.const, p=ARGS.p, iterations=ARGS.iterations, episodes=ARGS.episodes)
                        elif ARGS.agent == 'UCBVI':  # model-based
                            agent = UCBVI(dungeon, delta=ARGS.delta, iterations=ARGS.iterations, episodes=ARGS.episodes)
                        else:
                            print('ERROR: no such agent')

                        agent_rewards = []

                        # main loop
                        for episode in range(ARGS.episodes):
                            print(episode, '/', ARGS.episodes)
                            dungeon.state = random.randint(0, dungeon.length - 1)
                            total_reward = 0  # Score keeping
                            last_total = 0
                            for step in range(ARGS.iterations):
                                old_state = dungeon.state  # Store current state
                                action = agent.get_next_action(old_state, step)  # Query agent for the next action
                                new_state, reward = dungeon.take_action(action)  # Take action, get new state and reward
                                agent.update(old_state, new_state, action, reward, step, ARGS.iterations)  # Let the agent update internals

                                total_reward += reward  # Keep score
                                # if step % 250 == 0:  # Print out metadata every 250th iteration
                                #     performance = (total_reward - last_total) / 250.0
                                #     print(json.dumps({'step': step, 'performance': performance, 'total_reward': total_reward}))
                                #     last_total = total_reward

                                time.sleep(0.0001)  # Avoid spamming stdout too fast!

                            if ARGS.agent == 'UCBVI':
                                agent.UCB_Q_values(episode)

                            # Evaluate performance at the end of each episode
                            total_reward = 0
                            dungeon.reset()
                            for step in range(ARGS.iterations):
                                old_state = dungeon.state
                                action = agent.greedy_action(old_state, step)
                                new_state, reward = dungeon.take_action(action)  # Take action, get new state and reward
                                total_reward += reward
                                time.sleep(0.0001)  # Avoid spamming stdout too fast!

                            agent_rewards.append(total_reward)


                        # print("Final Q-table", agent.q_table)
                        # save q-table
                        if ARGS.agent == 'EPS_GREEDY' or ARGS.agent == 'UCBH':
                            np.save("outputs2\\" + str(ARGS.agent) + "_H_" + str(ARGS.iterations) + "_K_" + str(ARGS.episodes) + "_c_" + str(ARGS.const) + "_p_" + str(ARGS.p) + "_q_table.npy", agent.q_table)
                        elif ARGS.agent == 'UCBVI':
                            np.save("outputs\\" + str(ARGS.agent) + "_H_" + str(ARGS.iterations) + "_K_" + str(ARGS.episodes) + "_delta_" + str(ARGS.delta) + "_q_table.npy", agent.Q)
                        # save rewards results
                        if ARGS.agent == 'EPS_GREEDY' or ARGS.agent == 'UCBH':
                            np.save("outputs2\\" + str(ARGS.agent) + "_H_" + str(ARGS.iterations) + "_K_" + str(
                                ARGS.episodes) + "_c_" + str(
                                ARGS.const) + "_p_" + str(ARGS.p) + "_rewards.npy", np.array(agent_rewards))
                        elif ARGS.agent == 'UCBVI':
                            np.save("outputs\\" + str(ARGS.agent) + "_H_" + str(ARGS.iterations) + "_K_" + str(
                                ARGS.episodes) + "_delta_" + str(ARGS.delta) + "_rewards.npy", np.array(agent_rewards))

                        # plot results
                        plt.plot(np.arange(1, len(agent_rewards) + 1), agent_rewards,
                                 label=ARGS.agent)  # Plot some data on the (implicit) axes.
                        # plt.plot(np.arange(1, len(UCBH_rewards) + 1), UCBH_rewards,
                        #          label='UCB-V')  # Plot some data on the (implicit) axes.
                        plt.xlabel('Episode')
                        plt.ylabel('Cumulative reward')
                        plt.title(str(ARGS.agent) + '  |  H = ' + str(ARGS.iterations) + ' K = ' + str(ARGS.episodes))
                        plt.legend()

                        # saving figure
                        if ARGS.agent == 'EPS_GREEDY' or ARGS.agent == 'UCBH':
                            plt.savefig(r"outputs2\\" + str(ARGS.agent) + "_H_" + str(ARGS.iterations) + "_K_" + str(
                                ARGS.episodes) + "_c_" + str(ARGS.const) + "_p_" + str(ARGS.p) + ".png")
                        elif ARGS.agent == 'UCBVI':
                            plt.savefig(r"outputs\\" + str(ARGS.agent) + "_H_" + str(ARGS.iterations) + "_K_" + str(
                                ARGS.episodes) + "_delta_" + str(ARGS.delta) + ".png")

                        plt.clf()
                        # plt.show()


if __name__ == "__main__":
    now = datetime.datetime.now()
    print('\ncall - START ', now.hour, ':', now.minute, ':', now.second)
    main()
    now = datetime.datetime.now()
    print('\ncall - DONE ', now.hour, ':', now.minute, ':', now.second)
