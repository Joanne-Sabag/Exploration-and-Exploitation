from enums import *
import random
import numpy as np


class UCBH:
    def __init__(self, simulator, const=1, p=0.99, iterations=10, episodes=4000):
        self.q_table = iterations * np.ones(
            [2, simulator.length, iterations])  # Spreadsheet (Q-table) for rewards accounting
        self.n_visits = np.zeros(
            [2, simulator.length, iterations])  # number of visits on every state-action-step combination
        self.learning_rate = 1
        self.const = const
        self.iterations = iterations
        self.iota = np.log(simulator.length * 2 * iterations * episodes / p)

    def get_next_action(self, state, step):
        return self.greedy_action(state, step)

    def greedy_action(self, state, step):
        # Is FORWARD reward is bigger?
        if self.q_table[FORWARD][state][step] > self.q_table[BACKWARD][state][step]:
            return FORWARD
        # Is BACKWARD reward is bigger?
        elif self.q_table[BACKWARD][state][step] > self.q_table[FORWARD][state][step]:
            return BACKWARD
        # Rewards are equal, take random action
        return FORWARD if random.random() < 0.5 else BACKWARD

    def update(self, old_state, new_state, action, reward, step, iterations):
        self.n_visits[action][old_state][step] += 1
        t = self.n_visits[action][old_state][step]
        bonus = self.const * np.sqrt((self.iterations ** 3) * self.iota / t)
        # bonus = self.const * np.sqrt((self.iterations ** 3) * self.iota / (t ** 2))  # my-bonus 1
        # bonus = self.const * np.sqrt((self.iterations ** 3) * self.iota / np.sqrt(t))  # my-bonus 2
        # bonus = self.const * np.sqrt((self.iterations ** 2) * self.iota / t)  # my-bonus 3
        # bonus = self.const * np.sqrt((self.iterations ** 4) * self.iota / t)  # my-bonus 4
        self.learning_rate = (self.iterations + 1) / (self.iterations + t)

        # Old Q-table value
        old_value = self.q_table[action][old_state][step]
        # What would be our best next action?
        if step < iterations - 1:
            future_action = self.greedy_action(new_state, step + 1)

            # What is reward for the best next action?
            future_reward = min(iterations, self.q_table[future_action][new_state][step + 1])

        else:
            future_reward = 0  # ToDo: not sure


        # Main Q-table updating algorithm
        new_value = (1 - self.learning_rate) * old_value + self.learning_rate * (reward + future_reward + bonus)
        self.q_table[action][old_state][step] = new_value
