from enums import *
import numpy as np


class UCBVI:
    def __init__(self, simulator, delta=1 / 3, iterations=10, episodes=4000):
        self.env = simulator
        self.iterations = iterations
        self.episodes = episodes
        self.delta = delta
        self.buffer = {h: [] for h in range(self.iterations)}
        self.Nxay = {(s, a, s_): 0.0 for s in self.env.states.keys() for a in range(self.env.nAction) \
                     for s_ in self.env.states.keys()}
        self.Nxa = {(s, a): 0.0 for s in self.env.states.keys() for a in range(self.env.nAction)}
        self.N_ = {(h, s, a): 0.0 for h in range(self.iterations + 1) for s in self.env.states.keys() \
                   for a in range(self.env.nAction)}
        self.P = {(s, a, s_): 0.0 for s in self.env.states.keys() for a in \
                  range(self.env.nAction) for s_ in self.env.states.keys()}
        self.Q = {(h, s, a): self.iterations + 1 for h in range(self.iterations) for s in self.env.states.keys() \
                  for a in range(self.env.nAction)}

    def UCB_Q_values(self, episode):
        self.update_counts(episode)
        self.update_probability_transition()
        self.update_value_functions(episode)

    def update_counts(self, k):
        for d in self.buffer.values():
            s, a, r, s_, h = d[k][0], d[k][1], d[k][2], d[k][3], d[k][4]
            if s_ != None:
                self.Nxay[(s, a, s_)] += 1
            self.Nxa[(s, a)] += 1
            self.N_[(h, s, a)] += 1

    def update_probability_transition(self):
        for s in self.env.states.keys():
            for a in range(self.env.nAction):
                if self.Nxa[(s, a)] > 0:
                    for s_ in self.env.states.keys():
                        self.P[(s, a, s_)] = (self.Nxay[(s, a, s_)]) / (self.Nxa[(s, a)])

    def update_value_functions(self, k):
        V = {(h, s): 0.0 for s in self.env.states.keys() for h in range(self.iterations + 1)}
        for h in range(self.iterations - 1, -1, -1):
            for s in self.env.states.keys():
                for a in range(self.env.nAction):
                    if self.Nxa[(s, a)] > 0:
                        bonus = self.bonus_1(s, a)
                        PV = self.multiplyDictionaries(s, a, h, V)
                        R_sa = self.get_reward(s, a)
                        self.Q[(h, s, a)] = min(min(self.Q[(h, s, a)], self.iterations),
                                                R_sa + PV + bonus)
                    else:
                        self.Q[(h, s, a)] = self.iterations
                V[(h, s)] = max(np.array([self.Q[(h, s, a)] for a in range(self.env.nAction)]))

    def multiplyDictionaries(self, s, a, h, V):
        sums = 0.0
        for s_ in self.env.states.keys():
            sums += V[(h + 1, s_)] * self.P[(s, a, s_)]
        return sums

    def get_reward(self, s, a):
        if a == BACKWARD:  # BACKWARD: go back to the beginning, get small reward
            return self.env.small
        elif a == FORWARD:  # FORWARD: go up along the dungeon
            if s < self.env.length - 1:
                return 0
            else:
                return self.env.large

    def bonus_1(self, s, a):
        T = self.episodes * self.iterations
        L = np.log(5 * self.env.length * self.env.nAction * T / self.delta)
        return 7 * self.iterations * L * np.sqrt(1 / self.Nxa[(s, a)])

    def get_next_action(self, s, h):
        return self.greedy_action(s, h)

    def greedy_action(self, s, h):
        x = np.array([self.Q[(h, s, a)] for a in range(self.env.nAction)])
        action = np.random.choice(np.where(x == x.max())[0])
        return action

    def update(self, s, s_, a, r, h, H):
        self.buffer[h].append((s, a, r, s_, h))
