from enums import *
import random
import numpy as np


class Eps_greedy:
    def __init__(self, simulator, learning_rate=0.1, discount=1, exploration_rate=0.4, iterations=10):
        self.q_table = np.zeros([2, simulator.length, iterations])  # Spreadsheet (Q-table) for rewards accounting
        self.learning_rate = learning_rate  # How much we appreciate new q-value over current
        self.discount = discount  # How much we appreciate future reward over current
        self.exploration_rate = exploration_rate

    def get_next_action(self, state, step):
        if random.random() > self.exploration_rate:  # Explore (gamble) or exploit (greedy)
            return self.greedy_action(state, step)
        else:
            return self.random_action()

    def greedy_action(self, state, step):
        # Is FORWARD reward is bigger?
        if self.q_table[FORWARD][state][step] > self.q_table[BACKWARD][state][step]:
            return FORWARD
        # Is BACKWARD reward is bigger?
        elif self.q_table[BACKWARD][state][step] > self.q_table[FORWARD][state][step]:
            return BACKWARD
        # Rewards are equal, take random action
        return FORWARD if random.random() < 0.5 else BACKWARD

    def random_action(self):
        return FORWARD if random.random() < 0.5 else BACKWARD

    def update(self, old_state, new_state, action, reward, step, iterations):
        # Old Q-table value
        old_value = self.q_table[action][old_state][step]
        # # What would be our best next action?
        # future_action = self.greedy_action(new_state, step)
        # # What is reward for the best next action?
        # future_reward = self.q_table[future_action][new_state][step]

        # What would be our best next action?
        if step < iterations - 1:
            future_action = self.greedy_action(new_state, step + 1)

            # What is reward for the best next action?
            future_reward = min(iterations, self.q_table[future_action][new_state][step + 1])

        else:
            future_reward = 0  # ToDo: not sure

        # Main Q-table updating algorithm
        new_value = old_value + self.learning_rate * (reward + self.discount * future_reward - old_value)
        self.q_table[action][old_state][step] = new_value
