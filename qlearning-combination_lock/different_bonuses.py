import numpy as np
import matplotlib.pyplot as plt
from combination_lock_simulator import make_combination_lock
from UCBH import UCBH

S = 4
A = 5
H = 10

env_ucbh = make_combination_lock(epLen=H, nState=S, nAction=A)
env_opt = make_combination_lock(epLen=H, nState=S, nAction=A)

K = 100000  # number of episodes

agent_ucbh = UCBH(env_ucbh, K)
agent_ucbh_bonus1 = UCBH(env_ucbh, K, bonus=1)
agent_ucbh_bonus2 = UCBH(env_ucbh, K, bonus=2)
agent_ucbh_bonus3 = UCBH(env_ucbh, K, bonus=3)
agent_ucbh_bonus4 = UCBH(env_ucbh, K, bonus=4)
agent_ucbh_bonus5 = UCBH(env_ucbh, K, bonus=5)

agents = [agent_ucbh, agent_ucbh_bonus1, agent_ucbh_bonus2, agent_ucbh_bonus3, agent_ucbh_bonus4, agent_ucbh_bonus5]
bonuses = ['$b_t=c\sqrt{H^3\iota/t}$  (UCB-H)', '$b_t=c\sqrt{H^3\iota/t^2}$', '$b_t=c\sqrt{H^3\iota/\sqrt{t}}$', '$b_t=c\sqrt{H^4\iota/t}$', '$b_t=c\sqrt{H^2\iota/t}$', 'different initialization']
fig = plt.figure()
K_vec = np.array(range(K))
T = K_vec * H

for idx in range(len(agents)):
    env_ucbh.reset()
    agent_ucbh = agents[idx]

    V_pi_ucbh = []
    V_pi_opt = []

    for k in range(K):
        if k % 1000 == 0: print(k, '/', K)
        env_ucbh.reset()
        env_opt.reset()
        done_ucbh = 0

        while done_ucbh != 1:
            s_ucbh = env_ucbh.state
            h_ucbh = env_ucbh.timestep
            a_ucbh = agent_ucbh.act()

            s_opt = env_opt.state
            h_opt = env_opt.timestep
            a_opt = 0

            r_ucbh, s_ucbh_, done_ucbh = env_ucbh.advance(a_ucbh)
            agent_ucbh.learn(a_ucbh, r_ucbh, s_ucbh_)

            r_opt, s_opt_, done_opt = env_opt.advance(a_opt)

        # Evaluate performance at the end of each episode
        env_ucbh.reset()
        env_opt.reset()
        done_ucbh = 0
        V_pi_k_ucbh = 0
        V_pi_k_opt = 0
        while done_ucbh != 1:
            s_ucbh = env_ucbh.state
            h_ucbh = env_ucbh.timestep
            a_ucbh = agent_ucbh.greedy_action(s_ucbh, h_ucbh)

            s_opt = env_opt.state
            h_opt = env_opt.timestep
            a_opt = 0

            r_ucbh, s_ucbh_, done_ucbh = env_ucbh.advance(a_ucbh)
            r_opt, s_opt_, done_opt = env_opt.advance(a_opt)

            V_pi_k_ucbh += r_ucbh
            V_pi_k_opt += r_opt

        V_pi_ucbh.append(V_pi_k_ucbh)
        V_pi_opt.append(V_pi_k_opt)

    np.save("outputs_combLock\\bonuses_V_pi_ucbh" + "_H_" + str(H) + "_K_" + str(K) + "_S_" + str(S) + "_rewards.npy", np.array(V_pi_ucbh))
    np.save("outputs_combLock\\bonuses_V_pi_opt" + "_H_" + str(H) + "_K_" + str(K) + "_S_" + str(S) + "_rewards.npy", np.array(V_pi_opt))

    regret_ucbh = (np.cumsum(V_pi_opt) - np.cumsum(V_pi_ucbh))
    plt.plot(K_vec, regret_ucbh, label=str(bonuses[idx]))

plt.xlabel('Episode')
plt.ylabel('Total Regret')
plt.title('A = ' + str(A) + ' | S = ' + str(S) + "| H = " + str(H) + "| K = " + str(K))

plt.legend()

plt.savefig(r"outputs_combLock\\bonuses_" + "H_" + str(H) + "_K_" + str(K) + "_S_" + str(S) + "_A_" + str(A) + ".png")

plt.show()

