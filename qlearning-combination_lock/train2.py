import numpy as np
import matplotlib.pyplot as plt
from combination_lock_simulator import make_combination_lock
from UCBVI import UCBVI
from epsilon_greedy import EPS_GREEDY
from UCBH import UCBH
from getSize import getsize

S = 4
A = 5
H = 10

env_opt = make_combination_lock(epLen=H, nState=S, nAction=A)  # Simulation of optimal policy - always choose action=0
env_ucbh = make_combination_lock(epLen=H, nState=S, nAction=A)
env_eg = make_combination_lock(epLen=H, nState=S, nAction=A)
env_ucbvi = make_combination_lock(epLen=H, nState=S, nAction=A)

K = 100000  # number of episodes

results_list_ucbh =[]
results_list_eg = []
results_list_ucbvi = []

for experiment in range(10):
    agent_ucbh = UCBH(env_ucbh, K)
    agent_eg = EPS_GREEDY(env_eg, K)
    agent_ucbvi = UCBVI(env_ucbvi, K)

    V_pi_ucbh = []
    V_pi_eg = []
    V_pi_ucbvi = []
    V_pi_opt = []

    for k in range(K):
        if k % 1000 == 0: print(k, '/', K)
        env_ucbh.reset()
        env_eg.reset()
        env_ucbvi.reset()
        env_opt.reset()
        done_ucbh = 0

        while done_ucbh != 1:
            s_ucbh = env_ucbh.state
            h_ucbh = env_ucbh.timestep
            a_ucbh = agent_ucbh.act()

            s_eg = env_eg.state
            h_eg = env_eg.timestep
            a_eg = agent_eg.act()

            s_ucbvi = env_ucbvi.state
            h_ucbvi = env_ucbvi.timestep
            a_ucbvi = agent_ucbvi.act()

            s_opt = env_opt.state
            h_opt = env_opt.timestep
            a_opt = 0

            r_ucbh, s_ucbh_, done_ucbh = env_ucbh.advance(a_ucbh)
            agent_ucbh.learn(a_ucbh, r_ucbh, s_ucbh_)

            r_eg, s_eg_, done_eg = env_eg.advance(a_eg)
            agent_eg.learn(a_eg, r_eg)

            r_ucbvi, s_ucbvi_, done_ucbvi = env_ucbvi.advance(a_ucbvi)
            agent_ucbvi.update_buffer(s_ucbvi, a_ucbvi, r_ucbvi, s_ucbvi_, h_ucbvi)

            r_opt, s_opt_, done_opt = env_opt.advance(a_opt)

        agent_ucbvi.learn(k)

        # Evaluate performance at the end of each episode
        env_ucbh.reset()
        env_eg.reset()
        env_ucbvi.reset()
        env_opt.reset()
        done_ucbh = 0
        V_pi_k_ucbh = 0
        V_pi_k_eg = 0
        V_pi_k_ucbvi = 0
        V_pi_k_opt = 0
        while done_ucbh != 1:
            s_ucbh = env_ucbh.state
            h_ucbh = env_ucbh.timestep
            a_ucbh = agent_ucbh.greedy_action(s_ucbh, h_ucbh)

            s_eg = env_eg.state
            h_eg = env_eg.timestep
            a_eg = agent_eg.greedy_action(s_eg, h_eg)

            s_ucbvi = env_ucbvi.state
            h_ucbvi = env_ucbvi.timestep
            a_ucbvi = agent_ucbvi.greedy_action(s_ucbvi, h_ucbvi)

            s_opt = env_opt.state
            h_opt = env_opt.timestep
            a_opt = 0

            r_ucbh, s_ucbh_, done_ucbh = env_ucbh.advance(a_ucbh)
            r_eg, s_eg_, done_eg = env_eg.advance(a_eg)
            r_ucbvi, s_ucbvi_, done_ucbvi = env_ucbvi.advance(a_ucbvi)
            r_opt, s_opt_, done_opt = env_opt.advance(a_opt)

            V_pi_k_ucbh += r_ucbh
            V_pi_k_eg += r_eg
            V_pi_k_ucbvi += r_ucbvi
            V_pi_k_opt += r_opt

        V_pi_ucbh.append(V_pi_k_ucbh)
        V_pi_eg.append(V_pi_k_eg)
        V_pi_ucbvi.append(V_pi_k_ucbvi)
        V_pi_opt.append(V_pi_k_opt)

    np.save("outputs_combLock\\V_pi_ucbh" + "_H_" + str(H) + "_K_" + str(K) + "_S_" + str(S) + "_exp_" + str(
        experiment) + "_rewards.npy", np.array(V_pi_ucbh))
    np.save("outputs_combLock\\V_pi_eg" + "_H_" + str(H) + "_K_" + str(K) + "_S_" + str(S) + "_exp_" + str(
        experiment) + "_rewards.npy", np.array(V_pi_eg))
    np.save("outputs_combLock\\V_pi_ucbvi" + "_H_" + str(H) + "_K_" + str(K) + "_S_" + str(S) + "_exp_" + str(
        experiment) + "_rewards.npy", np.array(V_pi_ucbvi))
    np.save("outputs_combLock\\V_pi_opt" + "_H_" + str(H) + "_K_" + str(K) + "_S_" + str(S) + "_exp_" + str(
        experiment) + "_rewards.npy", np.array(V_pi_opt))

    print('\n Objects\' sizes in Bytes:')
    print('ucbh agent:', getsize(agent_ucbh.Q))
    print('ucbh env:', getsize(env_ucbh))
    print('eg agent:', getsize(agent_eg))
    print('eg env:', getsize(env_eg))
    print('ucbvi agent:', getsize(agent_ucbvi))
    print('ucbvi env:', getsize(env_ucbvi))

    results_list_ucbh.append(np.cumsum(V_pi_opt) - np.cumsum(V_pi_ucbh))
    results_list_eg.append(np.cumsum(V_pi_opt) - np.cumsum(V_pi_eg))
    results_list_ucbvi.append(np.cumsum(V_pi_opt) - np.cumsum(V_pi_ucbvi))

fig = plt.figure()
K_vec = np.array(range(K))
T = K_vec * H

ucbh_regret_bound = H ** 2 * np.sqrt(S * A * T)
eps_regret_bound = np.minimum(T, np.ones(T.shape) * A ** (H / 2))
ucbvi_regret_bound = H * np.sqrt(S * A * T)

results_list_ucbh = np.array(results_list_ucbh)
regret_ucbh = np.mean(results_list_ucbh, axis=0)
results_list_eg = np.array(results_list_eg)
regret_eg = np.mean(results_list_eg, axis=0)
results_list_ucbvi = np.array(results_list_ucbvi)
regret_ucbvi = np.mean(results_list_ucbvi, axis=0)

plt.plot(K_vec, ucbh_regret_bound, label='$\sqrt{H^4SAT}$', color='green', linestyle='dashed')
plt.plot(K_vec, eps_regret_bound, label='$min(T,A^{H/2})$', color='orange', linestyle='dashed')
plt.plot(K_vec, ucbvi_regret_bound, label='$\sqrt{H^2SAT}$', color='blue', linestyle='dashed')

plt.plot(K_vec, regret_ucbh, label='UCB-H', color='green')
plt.plot(K_vec, regret_eg, label='$\epsilon$-Greedy', color='orange')
plt.plot(K_vec, regret_ucbvi, label='UCB-VI-H', color='blue')

plt.xlabel('Episode')
plt.ylabel('Total Regret')
plt.title('A = ' + str(A) + ' | S = ' + str(S) + "| H = " + str(H) + "| K = " + str(K))

plt.legend()

plt.savefig(r"outputs_combLock\\regret_" + "H_" + str(H) + "_K_" + str(K) + "_S_" + str(S) + "_A_" + str(A) + ".png")

plt.show()
