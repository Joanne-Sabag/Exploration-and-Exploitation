import numpy as np
import random
import copy


class UCBH(object):
    def __init__(self, env, K, c=0.01, p=0.01, bonus=0):
        self.env = env
        self.K = K
        self.c = c
        self.p = p
        self.iota = np.log(self.env.nState * self.env.nAction * self.env.epLen * self.K / self.p)
        self.learning_rate = 1
        self.bonus = bonus
        self.buffer = {h: [] for h in range(self.env.epLen)}
        self.N = {(h, s, a): 0.0 for h in range(self.env.epLen + 1) for s in self.env.states.keys() \
                  for a in range(self.env.nAction)}
        if bonus == 5:
            init_value = 1 + self.env.epLen + c * np.sqrt(self.env.epLen**3 * self.iota)
            self.Q = {(h, s, a): init_value for h in range(self.env.epLen) for s in self.env.states.keys() \
                      for a in range(self.env.nAction)}
        else:
            self.Q = {(h, s, a): self.env.epLen for h in range(self.env.epLen) for s in self.env.states.keys() \
                      for a in range(self.env.nAction)}


    def act(self):
        s = self.env.state
        h = self.env.timestep
        return self.greedy_action(s, h)

    def greedy_action(self, s, h):
        x = np.array([self.Q[(h, s, a)] for a in range(self.env.nAction)])
        return self.env.argmax(x)

    def learn(self, action, reward, s_):
        self.update_value_functions(action, reward, s_)

    def update_value_functions(self, action, reward, s_):
        self.N[(self.env.timestep - 1, self.env.prev_state, action)] += 1
        t = self.N[(self.env.timestep - 1, self.env.prev_state, action)]
        bonus = self.my_bonus(t)
        self.learning_rate = (self.env.epLen + 1) / (self.env.epLen + t)

        old_value = self.Q[(self.env.timestep - 1, self.env.prev_state, action)]  # the timestep and state were incremented while acting
        if self.env.timestep < self.env.epLen:
            future_action = self.greedy_action(s_, self.env.timestep)
            future_reward = min(self.env.epLen, self.Q[(self.env.timestep, s_, future_action)])
        else:
            future_reward = 0

        new_value = (1 - self.learning_rate) * old_value + self.learning_rate * (reward + future_reward + bonus)
        self.Q[(self.env.timestep - 1, self.env.prev_state, action)] = new_value

    def my_bonus(self, t):
        if self.bonus == 0:
            return self.c * np.sqrt((self.env.epLen ** 3) * self.iota / t)
        elif self.bonus == 1:
            return self.c * np.sqrt((self.env.epLen ** 3) * self.iota / (t**2))
        elif self.bonus == 2:
            return self.c * np.sqrt((self.env.epLen ** 3) * self.iota / np.sqrt(t))
        elif self.bonus == 3:
            return self.c * np.sqrt((self.env.epLen ** 4) * self.iota / t)
        elif self.bonus == 4:
            return self.c * np.sqrt((self.env.epLen ** 2) * self.iota / t)
        else:
            return self.c * np.sqrt((self.env.epLen ** 3) * self.iota / t)
